{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52e55666",
   "metadata": {},
   "source": [
    "### Installing packages and pulling the model\n",
    "This cell runs shell commands that install the MONAI package (where all models are stored), then download the lung_nodule_ct_detection model specifically.  The model and its accompanying files will be downloaded to the /bundles/lung_nodle_ct_detection/ subdirectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4424985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: monai[fire] in C:\\Users\\eocon\\miniconda3\\envs\\scale\\Lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy<3.0,>=1.24 in C:\\Users\\eocon\\miniconda3\\envs\\scale\\Lib\\site-packages (from monai[fire]) (2.4.2)\n",
      "Requirement already satisfied: torch>=2.4.1 in C:\\Users\\eocon\\miniconda3\\envs\\scale\\Lib\\site-packages (from monai[fire]) (2.10.0)\n",
      "Requirement already satisfied: fire in C:\\Users\\eocon\\miniconda3\\envs\\scale\\Lib\\site-packages (from monai[fire]) (0.7.1)\n",
      "Requirement already satisfied: filelock in C:\\Users\\eocon\\miniconda3\\envs\\scale\\Lib\\site-packages (from torch>=2.4.1->monai[fire]) (3.20.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in C:\\Users\\eocon\\miniconda3\\envs\\scale\\Lib\\site-packages (from torch>=2.4.1->monai[fire]) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in C:\\Users\\eocon\\miniconda3\\envs\\scale\\Lib\\site-packages (from torch>=2.4.1->monai[fire]) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in C:\\Users\\eocon\\miniconda3\\envs\\scale\\Lib\\site-packages (from torch>=2.4.1->monai[fire]) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in C:\\Users\\eocon\\miniconda3\\envs\\scale\\Lib\\site-packages (from torch>=2.4.1->monai[fire]) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in C:\\Users\\eocon\\miniconda3\\envs\\scale\\Lib\\site-packages (from torch>=2.4.1->monai[fire]) (2026.2.0)\n",
      "Requirement already satisfied: setuptools in C:\\Users\\eocon\\miniconda3\\envs\\scale\\Lib\\site-packages (from torch>=2.4.1->monai[fire]) (80.10.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in C:\\Users\\eocon\\miniconda3\\envs\\scale\\Lib\\site-packages (from sympy>=1.13.3->torch>=2.4.1->monai[fire]) (1.3.0)\n",
      "Requirement already satisfied: termcolor in C:\\Users\\eocon\\miniconda3\\envs\\scale\\Lib\\site-packages (from fire->monai[fire]) (3.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in C:\\Users\\eocon\\miniconda3\\envs\\scale\\Lib\\site-packages (from jinja2->torch>=2.4.1->monai[fire]) (3.0.3)\n",
      "2026-02-17 11:23:40,083 - INFO - --- input summary of monai.bundle.scripts.download ---\n",
      "2026-02-17 11:23:40,083 - INFO - > name: 'lung_nodule_ct_detection'\n",
      "2026-02-17 11:23:40,083 - INFO - > bundle_dir: 'bundles/'\n",
      "2026-02-17 11:23:40,083 - INFO - > source: 'monaihosting'\n",
      "2026-02-17 11:23:40,083 - INFO - > remove_prefix: 'monai_'\n",
      "2026-02-17 11:23:40,083 - INFO - > progress: True\n",
      "2026-02-17 11:23:40,083 - INFO - ---\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching 20 files:   0%|          | 0/20 [00:00<?, ?it/s]\n",
      "Fetching 20 files: 100%|██████████| 20/20 [00:00<00:00, 1849.91it/s]\n"
     ]
    }
   ],
   "source": [
    "!pip install monai[fire]\n",
    "!python -m monai.bundle download \"lung_nodule_ct_detection\" --bundle_dir \"bundles/\"  # model and accompanying files will be in the /bundles folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9773bb49",
   "metadata": {},
   "source": [
    "----------------------------------------\n",
    "### Loading the model\n",
    "MONAI provides a \"config\" file that tells you how to work with the model.  Specifies:\n",
    "1. Model architecture\n",
    "2. Data preprocessing specifications\n",
    "3. Inference logic - the model wants a patch of the 3D image of size 512x512x192\n",
    "4. Output format - how the model will output its prediction (coordinates of a bounding box surrounding the tumor)\n",
    "\n",
    "So, what we will do is first parse the config file and get this important info.  Then, we can build the architecture specified in the config file to instantiate the model.  Finally, we load the trained weights into the architecture we instantiated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "760d7c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from monai.bundle import ConfigParser\n",
    "\n",
    "# !!! YOU NEED TO CHANGE THIS TO YOUR ROOT PATH !!!\n",
    "bundle_root = r\"C:\\Users\\eocon\\Documents\\scalable project\\CS6423_knowledge_distillation_project\\bundles\\lung_nodule_ct_detection\" \n",
    "\n",
    "#  defines paths to the bundle and config file\n",
    "config_file = os.path.join(bundle_root, \"configs\", \"inference.json\")\n",
    "model_path = os.path.join(bundle_root, \"models\", \"model.pt\")\n",
    "\n",
    "# parses the config file\n",
    "parser = ConfigParser()\n",
    "parser.read_config(config_file)\n",
    "\n",
    "# defines a new element in the parsed config file - the path to our bundle folder\n",
    "parser[\"bundle_root\"] = bundle_root\n",
    "\n",
    "#print(parser)  # see the dictionary inside if you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "143b5bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and preprocessing pipeline ready.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "monai.transforms.spatial.dictionary Orientationd.__init__:labels: Current default value of argument `labels=(('L', 'R'), ('P', 'A'), ('I', 'S'))` was changed in version None from `labels=(('L', 'R'), ('P', 'A'), ('I', 'S'))` to `labels=None`. Default value changed to None meaning that the transform now uses the 'space' of a meta-tensor, if applicable, to determine appropriate axis labels.\n"
     ]
    }
   ],
   "source": [
    "# instantiates the network - ie, creates a skeleton architecture as described in the config file\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "network = parser.get_parsed_content(\"network_def\").to(device)\n",
    "\n",
    "# loads the trained weights into the skeleton architecture\n",
    "network.load_state_dict(torch.load(model_path, map_location=device))\n",
    "network.eval()\n",
    "\n",
    "# load the preprocessing transforms\n",
    "# this is a series of transforms that loads the image, confirms shape, normalizes, etc\n",
    "preprocessing = parser.get_parsed_content(\"preprocessing\")\n",
    "\n",
    "print(\"Model and preprocessing pipeline ready.\")\n",
    "#print(preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f932fc9b",
   "metadata": {},
   "source": [
    "--------------------------------\n",
    "### Investigating the model\n",
    "We need to be aware of the actual architecture we're working with, number of parameters, etc.  These cells will output information on this.  We will first look at the named sub-networks that are within the model.  These are:\n",
    "1. feature_extractor: extracts info from the scan, turning the raw input into meaningful features\n",
    "2. classification_head: this will look at the extracted features and classify \"is there a nodule/tumor\"\n",
    "3. regression_head: this will actually define a bounding box around the nodule/tumor, \"where exactly is the nodule\"\n",
    "\n",
    "We'll look at the parameters and layers in each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c32840aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_extractor\n",
      "classification_head\n",
      "regression_head\n",
      "\n",
      "Total Parameters in the entire Lung Nodule Detector: 20,902,741\n"
     ]
    }
   ],
   "source": [
    "# the model has named sub-networks within - the feature_extractor, classification_head, and regression_head.  \n",
    "for name, child in network.named_children():\n",
    "    print(name)\n",
    "\n",
    "total_params = sum(p.numel() for p in network.parameters())\n",
    "print(f\"\\nTotal Parameters in the entire Lung Nodule Detector: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "467d6a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component                 | Type                 | Parameters     \n",
      "-----------------------------------------------------------------\n",
      "Feature Extractor         | FPN                  | 6,595,648\n",
      "Classification Head       | Sub-network          | 7,101,699\n",
      "Regression Head           | Sub-network          | 7,205,394\n",
      "-----------------------------------------------------------------\n",
      "TOTAL                     |                      | 20,902,741\n"
     ]
    }
   ],
   "source": [
    "# iterates through the sub-networks and shows how many parameters are in each\n",
    "def print_detection_summary(model):\n",
    "    print(f\"{'Component':<25} | {'Type':<20} | {'Parameters':<15}\")\n",
    "    print(\"-\" * 65)\n",
    "    \n",
    "    # 2. Feature Extractor (FPN)\n",
    "    params = sum(p.numel() for p in model.feature_extractor.parameters())\n",
    "    print(f\"{'Feature Extractor':<25} | {'FPN':<20} | {params:,}\")\n",
    "    \n",
    "    # 3. Detection Heads\n",
    "    # RetinaNet has a class_head and a box_head\n",
    "    params_class = sum(p.numel() for p in model.classification_head.parameters())\n",
    "    params_box = sum(p.numel() for p in model.regression_head.parameters())\n",
    "    print(f\"{'Classification Head':<25} | {'Sub-network':<20} | {params_class:,}\")\n",
    "    print(f\"{'Regression Head':<25} | {'Sub-network':<20} | {params_box:,}\")\n",
    "    \n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(\"-\" * 65)\n",
    "    print(f\"{'TOTAL':<25} | {'':<20} | {total_params:,}\")\n",
    "\n",
    "print_detection_summary(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be7c0b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Feature Extractor ---\n",
      "BackboneWithFPN(\n",
      "  (body): IntermediateLayerGetter(\n",
      "    (conv1): Conv3d(1, 64, kernel_size=(7, 7, 7), stride=(2, 2, 1), padding=(3, 3, 3), bias=False)\n",
      "    (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (act): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool3d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): ResNetBottleneck(\n",
      "        (conv1): Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "          (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): ResNetBottleneck(\n",
      "        (conv1): Conv3d(256, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): ResNetBottleneck(\n",
      "        (conv1): Conv3d(256, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): ResNetBottleneck(\n",
      "        (conv1): Conv3d(256, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "        (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv3d(128, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv3d(256, 512, kernel_size=(1, 1, 1), stride=(2, 2, 2))\n",
      "          (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): ResNetBottleneck(\n",
      "        (conv1): Conv3d(512, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv3d(128, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): ResNetBottleneck(\n",
      "        (conv1): Conv3d(512, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv3d(128, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): ResNetBottleneck(\n",
      "        (conv1): Conv3d(512, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv3d(128, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fpn): FeaturePyramidNetwork(\n",
      "    (inner_blocks): ModuleList(\n",
      "      (0): Conv3d(256, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "      (1): Conv3d(512, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "    )\n",
      "    (layer_blocks): ModuleList(\n",
      "      (0-1): 2 x Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    )\n",
      "    (extra_blocks): LastLevelMaxPool(\n",
      "      (maxpool): MaxPool3d(kernel_size=1, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# To see the types of layers in the feature extractor:\n",
    "print(\"--- Feature Extractor ---\")\n",
    "print(network.feature_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b3573ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Classification Head ---\n",
      "RetinaNetClassificationHead(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    (1): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "    (2): ReLU()\n",
      "    (3): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    (4): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "    (5): ReLU()\n",
      "    (6): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    (7): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "    (8): ReLU()\n",
      "    (9): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    (10): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "    (11): ReLU()\n",
      "  )\n",
      "  (cls_logits): Conv3d(256, 3, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      ")\n",
      "\n",
      "--- Regression Head ---\n",
      "RetinaNetRegressionHead(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    (1): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "    (2): ReLU()\n",
      "    (3): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    (4): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "    (5): ReLU()\n",
      "    (6): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    (7): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "    (8): ReLU()\n",
      "    (9): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    (10): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "    (11): ReLU()\n",
      "  )\n",
      "  (bbox_reg): Conv3d(256, 18, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# To see the Classification Head architecture:\n",
    "print(\"--- Classification Head ---\")\n",
    "print(network.classification_head)\n",
    "\n",
    "# To see the Regression Head architecture:\n",
    "print(\"\\n--- Regression Head ---\")\n",
    "print(network.regression_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bcef5e",
   "metadata": {},
   "source": [
    "-------------------------\n",
    "### Running Inference on example data\n",
    "Now we need to actually load some example data and run it through the model.  From this, we can see how our laptops handle the model, inference time, etc. to start benchmarking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "310d8b19",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "Cannot locate class or function path: 'scripts.detection_inferer.RetinaNetInferer'.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m parser.set(\u001b[33m\"\u001b[39m\u001b[33mdataset#data\u001b[39m\u001b[33m\"\u001b[39m, [{\u001b[33m\"\u001b[39m\u001b[33mimage\u001b[39m\u001b[33m\"\u001b[39m: input_image}])\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Initialize the inference pipeline\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m inferer = \u001b[43mparser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_parsed_content\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minferer\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Execute\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m     12\u001b[39m     \u001b[38;5;66;03m# Note: Lung detection often requires specific preprocessing (resampling to 0.7x0.7x1.25mm)\u001b[39;00m\n\u001b[32m     13\u001b[39m     \u001b[38;5;66;03m# The 'preprocessing' transform in the config handles this.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eocon\\miniconda3\\envs\\scale\\Lib\\site-packages\\monai\\bundle\\config_parser.py:290\u001b[39m, in \u001b[36mConfigParser.get_parsed_content\u001b[39m\u001b[34m(self, id, **kwargs)\u001b[39m\n\u001b[32m    288\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mlazy\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m    289\u001b[39m     \u001b[38;5;28mself\u001b[39m.parse(reset=\u001b[38;5;129;01mnot\u001b[39;00m kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mlazy\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[32m--> \u001b[39m\u001b[32m290\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mref_resolver\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_resolved_content\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eocon\\miniconda3\\envs\\scale\\Lib\\site-packages\\monai\\bundle\\reference_resolver.py:193\u001b[39m, in \u001b[36mReferenceResolver.get_resolved_content\u001b[39m\u001b[34m(self, id, **kwargs)\u001b[39m\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_resolved_content\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mid\u001b[39m: \u001b[38;5;28mstr\u001b[39m, **kwargs: Any) -> ConfigExpression | \u001b[38;5;28mstr\u001b[39m | Any | \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    182\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    183\u001b[39m \u001b[33;03m    Get the resolved ``ConfigItem`` by id.\u001b[39;00m\n\u001b[32m    184\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    191\u001b[39m \n\u001b[32m    192\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_resolve_one_item\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eocon\\miniconda3\\envs\\scale\\Lib\\site-packages\\monai\\bundle\\reference_resolver.py:171\u001b[39m, in \u001b[36mReferenceResolver._resolve_one_item\u001b[39m\u001b[34m(self, id, waiting_list, **kwargs)\u001b[39m\n\u001b[32m    169\u001b[39m \u001b[38;5;66;03m# save the resolved result into `resolved_content` to recursively resolve others\u001b[39;00m\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, ConfigComponent):\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m     \u001b[38;5;28mself\u001b[39m.resolved_content[\u001b[38;5;28mid\u001b[39m] = \u001b[43mitem\u001b[49m\u001b[43m.\u001b[49m\u001b[43minstantiate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m kwargs.get(\u001b[33m\"\u001b[39m\u001b[33minstantiate\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01melse\u001b[39;00m item\n\u001b[32m    172\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, ConfigExpression):\n\u001b[32m    173\u001b[39m     run_eval = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33meval_expr\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eocon\\miniconda3\\envs\\scale\\Lib\\site-packages\\monai\\bundle\\config_item.py:292\u001b[39m, in \u001b[36mConfigComponent.instantiate\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    290\u001b[39m args = \u001b[38;5;28mself\u001b[39m.resolve_args()\n\u001b[32m    291\u001b[39m args.update(kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minstantiate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eocon\\miniconda3\\envs\\scale\\Lib\\site-packages\\monai\\utils\\module.py:230\u001b[39m, in \u001b[36minstantiate\u001b[39m\u001b[34m(__path, __mode, **kwargs)\u001b[39m\n\u001b[32m    228\u001b[39m component = locate(__path) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(__path, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m __path\n\u001b[32m    229\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m component \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m230\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCannot locate class or function path: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m__path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    231\u001b[39m m = look_up_option(__mode, CompInitMode)\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: Cannot locate class or function path: 'scripts.detection_inferer.RetinaNetInferer'."
     ]
    }
   ],
   "source": [
    "## NOT WORKING YET\n",
    "\n",
    "# Replace with your actual image path\n",
    "input_image = \"IMG_0002.nii.gz\"\n",
    "parser.set(\"dataset#data\", [{\"image\": input_image}])\n",
    "\n",
    "# Initialize the inference pipeline\n",
    "inferer = parser.get_parsed_content(\"inferer\")\n",
    "\n",
    "# Execute\n",
    "with torch.no_grad():\n",
    "    # Note: Lung detection often requires specific preprocessing (resampling to 0.7x0.7x1.25mm)\n",
    "    # The 'preprocessing' transform in the config handles this.\n",
    "    preprocessing = parser.get_parsed_content(\"preprocessing\")\n",
    "    data = preprocessing({\"image\": input_image})\n",
    "    output = inferer(inputs=data[\"image\"].unsqueeze(0), network=network)\n",
    "    \n",
    "    print(\"Detected Boxes:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7def06e2",
   "metadata": {},
   "source": [
    "### Attempts to load data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa2cf4a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-17 10:50:29,870:INFO:Calling getSeries with parameters {'Collection': 'LIDC-IDRI', 'PatientID': 'LIDC-IDRI-0001'}\n",
      "2026-02-17 10:50:32,972:INFO:Directory './test_data' already exists.\n",
      "2026-02-17 10:50:32,973:WARNING:Series 1.3.6.1.4.1.14519.5.2.1.6279.6001.141365756818074696859567662357 already downloaded and unzipped.\n",
      "2026-02-17 10:50:32,973:INFO:Found 0 previously downloaded series.\n",
      "2026-02-17 10:50:32,973:INFO:Attempting to download 1 new series.\n",
      "2026-02-17 10:50:32,980:INFO:Downloading... https://nbia.cancerimagingarchive.net/nbia-api/services/v4/getImage?NewFileNames=Yes&SeriesInstanceUID=1.3.6.1.4.1.14519.5.2.1.6279.6001.179049373636438705059720603192\n",
      "2026-02-17 10:51:48,961:INFO:Downloaded 1 out of 1 targeted series.\n",
      "0 failed to download.\n",
      "0 were previously downloaded.\n"
     ]
    }
   ],
   "source": [
    "from tcia_utils import nbia\n",
    "\n",
    "# LIDC-IDRI collection name\n",
    "collection = \"LIDC-IDRI\"\n",
    "\n",
    "# Get a list of series for the first patient\n",
    "series_data = nbia.getSeries(collection=collection, patientId=\"LIDC-IDRI-0001\")\n",
    "\n",
    "# Download the first CT scan series (this will download a folder of DICOMs)\n",
    "# 'number=1' ensures we only get one scan for testing\n",
    "nbia.downloadSeries(series_data, number=1, path=\"./test_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bf24c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 files in directory.\n",
      "First 3 files: ['1-1.dcm', '1-2.dcm', 'LICENSE']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'dicom2nifti.dicom_series_to_nifti(dicom_directory, output_file)\\nprint(f\"Conversion complete: {output_file}\")'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dicom2nifti\n",
    "import os\n",
    "\n",
    "dicom_directory = \"C:/Users/eocon/Documents/scalable project/test_data/1.3.6.1.4.1.14519.5.2.1.6279.6001.141365756818074696859567662357\"\n",
    "output_file = \"./sample_lung_ct.nii.gz\"\n",
    "\n",
    "files = os.listdir(dicom_directory)\n",
    "print(f\"Found {len(files)} files in directory.\")\n",
    "print(f\"First 3 files: {files[:3]}\")\n",
    "\n",
    "'''dicom2nifti.dicom_series_to_nifti(dicom_directory, output_file)\n",
    "print(f\"Conversion complete: {output_file}\")'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scale",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
